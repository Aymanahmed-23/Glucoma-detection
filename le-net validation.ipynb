{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9666c4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: monai==1.3.0 in c:\\users\\victus\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\victus\\anaconda3\\lib\\site-packages (from monai==1.3.0) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.9 in c:\\users\\victus\\anaconda3\\lib\\site-packages (from monai==1.3.0) (2.7.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\victus\\anaconda3\\lib\\site-packages (from torch>=1.9->monai==1.3.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\victus\\anaconda3\\lib\\site-packages (from torch>=1.9->monai==1.3.0) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\victus\\anaconda3\\lib\\site-packages (from torch>=1.9->monai==1.3.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\victus\\anaconda3\\lib\\site-packages (from torch>=1.9->monai==1.3.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\victus\\anaconda3\\lib\\site-packages (from torch>=1.9->monai==1.3.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\victus\\anaconda3\\lib\\site-packages (from torch>=1.9->monai==1.3.0) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\victus\\anaconda3\\lib\\site-packages (from torch>=1.9->monai==1.3.0) (75.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\victus\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.9->monai==1.3.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\victus\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.9->monai==1.3.0) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install monai==1.3.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cd2b083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ea61415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3743e2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in c:\\users\\victus\\anaconda3\\lib\\site-packages (0.22.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\victus\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.7.0 in c:\\users\\victus\\anaconda3\\lib\\site-packages (from torchvision) (2.7.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\victus\\anaconda3\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\victus\\anaconda3\\lib\\site-packages (from torch==2.7.0->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\victus\\anaconda3\\lib\\site-packages (from torch==2.7.0->torchvision) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\victus\\anaconda3\\lib\\site-packages (from torch==2.7.0->torchvision) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\victus\\anaconda3\\lib\\site-packages (from torch==2.7.0->torchvision) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\victus\\anaconda3\\lib\\site-packages (from torch==2.7.0->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\victus\\anaconda3\\lib\\site-packages (from torch==2.7.0->torchvision) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\victus\\anaconda3\\lib\\site-packages (from torch==2.7.0->torchvision) (75.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\victus\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch==2.7.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\victus\\anaconda3\\lib\\site-packages (from jinja2->torch==2.7.0->torchvision) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "049aa500",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2ac36ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "origa_images_folder = \"ORIGA/ORIGA/Images\"      # folder with all ORIGA images (.jpg)\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d2c378a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "origa_mat_file = \"ORIGA/ORIGA/OrigaList.mat\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d46bb040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ORIGA']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.listdir(\"ORIGA\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1c3d235e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'Origa'])\n"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "\n",
    "origa_mat_file = \"ORIGA/ORIGA/OrigaList.mat\"\n",
    "mat = scipy.io.loadmat(origa_mat_file)\n",
    "\n",
    "print(mat.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "002b4834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[('Eye', 'O'), ('Filename', 'O'), ('ExpCDR', 'O'), ('Set', 'O'), ('Glaucoma', 'O')]\n",
      "(1, 650)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "origa = mat['Origa']\n",
    "print(type(origa))          # Usually <class 'numpy.ndarray'>\n",
    "print(origa.dtype)          # To see the structured array fields\n",
    "print(origa.shape)          # Usually (1, 1) or similar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7ebbde5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Eye', 'Filename', 'ExpCDR', 'Set', 'Glaucoma')\n"
     ]
    }
   ],
   "source": [
    "print(origa.dtype.names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2715992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = origa['Filename'][0, 0]     # (N, 1) array of strings\n",
    "labels = origa['Glaucoma'][0, 0]        # (N, 1) array of 0/1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "32fbcc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [str(f[0]) for f in filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "51eadb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to integers\n",
    "labels = labels.flatten().astype(int).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ba69c188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define full path to images\n",
    "image_dir = \"E:/Glucoma detection/U-Net/ORIGA/Images\"\n",
    "image_paths = [os.path.join(image_dir, fname + \".jpg\") for fname in filenames]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "00d331b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class ORIGADataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"L\")  # convert to grayscale for LeNet\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ce9d12e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Eye', 'O'), ('Filename', 'O'), ('ExpCDR', 'O'), ('Set', 'O'), ('Glaucoma', 'O')]\n",
      "(1, 650)\n"
     ]
    }
   ],
   "source": [
    "mat = scipy.io.loadmat(\"ORIGA/ORIGA/OrigaList.mat\")\n",
    "print(mat['Origa'].dtype)\n",
    "print(mat['Origa'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "67bb17b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw filename from .mat: '001.jpg'\n"
     ]
    }
   ],
   "source": [
    "origa_data = mat['Origa']\n",
    "num_samples = origa_data.shape[1]\n",
    "\n",
    "filenames = []\n",
    "labels = []\n",
    "\n",
    "# From your earlier loop\n",
    "for i in range(num_samples):\n",
    "    sample = origa_data[0, i]\n",
    "    raw_name = sample[1][0]  # This is where it might be breaking\n",
    "\n",
    "    print(f\"Raw filename from .mat: '{raw_name}'\")  # Debug line\n",
    "    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5c5ef687",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = []\n",
    "final_labels = []\n",
    "\n",
    "for i in range(num_samples):\n",
    "    sample = origa_data[0, i]\n",
    "    raw_name = sample[1][0].strip()  # This is '001.jpg'\n",
    "    glaucoma = sample[4][0][0]       # This is 0 or 1\n",
    "\n",
    "    # Construct the full path directly\n",
    "    full_path = os.path.join(image_dir, raw_name)\n",
    "\n",
    "    if os.path.exists(full_path):\n",
    "        image_paths.append(full_path)\n",
    "        final_labels.append(glaucoma)\n",
    "    else:\n",
    "        print(f\"Missing image: {full_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5081f248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matched images: 650\n",
      "First 5: ['ORIGA\\\\ORIGA\\\\Images\\\\001.jpg', 'ORIGA\\\\ORIGA\\\\Images\\\\002.jpg', 'ORIGA\\\\ORIGA\\\\Images\\\\003.jpg', 'ORIGA\\\\ORIGA\\\\Images\\\\004.jpg', 'ORIGA\\\\ORIGA\\\\Images\\\\005.jpg'] [0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Total matched images:\", len(image_paths))\n",
    "print(\"First 5:\", image_paths[:5], final_labels[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6d51cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we go for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "20d36a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class OrigaDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')  # convert grayscale to RGB by repeating channels\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Transforms for ResNet input\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # ResNet input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],  # ImageNet mean\n",
    "                         [0.229, 0.224, 0.225])  # ImageNet std\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4712f2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = OrigaDataset(image_paths, final_labels, transform=transform)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Move inputs and labels in training loop:\n",
    "inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "374c1be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision.models import resnet18\n",
    "model = resnet18(pretrained=True)\n",
    "# Load pretrained ResNet18\n",
    "\n",
    "\n",
    "# Freeze early layers (optional)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True  # Fine-tune all layers\n",
    "\n",
    "\n",
    "# Replace the final fully connected layer for 2 classes (glaucoma vs normal)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 2)  # binary classification\n",
    "\n",
    "# Unfreeze the new fc layer parameters\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bda1e8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "82461669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.5983\n",
      "Epoch 2/10, Loss: 0.5482\n",
      "Epoch 3/10, Loss: 0.5181\n",
      "Epoch 4/10, Loss: 0.5072\n",
      "Epoch 5/10, Loss: 0.5039\n",
      "Epoch 6/10, Loss: 0.4609\n",
      "Epoch 7/10, Loss: 0.4689\n",
      "Epoch 8/10, Loss: 0.5305\n",
      "Epoch 9/10, Loss: 0.4741\n",
      "Epoch 10/10, Loss: 0.4590\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "770527e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 76.15%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a501b6f9",
   "metadata": {},
   "source": [
    "# ROI Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d0b0c297",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[93], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def extract_roi(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Use Gaussian blur to smooth and threshold\n",
    "    blurred = cv2.GaussianBlur(gray, (7, 7), 0)\n",
    "    _, thresh = cv2.threshold(blurred, 15, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Assume largest contour is optic disc\n",
    "    c = max(contours, key=cv2.contourArea)\n",
    "    x, y, w, h = cv2.boundingRect(c)\n",
    "    \n",
    "    roi = img[y:y+h, x:x+w]\n",
    "    roi_resized = cv2.resize(roi, (32, 32))  # Resize for ResNet\n",
    "    return Image.fromarray(cv2.cvtColor(roi_resized, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb94f6c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
